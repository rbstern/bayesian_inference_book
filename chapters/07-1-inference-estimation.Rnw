\section{Inferência Bayesiana}
\subsection{Estimação}

O problema de estimação consiste em 
escolher a partir dos dados 
um valor próximo ao parâmetro do modelo estatístico.
O valor escolhido é chamado de estimador do parâmetro e
será denotado por $\hat{\theta}$.
Nesta seção, tomaremos que 
$\mathcal{A}_{*} = \Theta \subset \mathbb{R}^{k}$.
Para capturar a ideia de que
o valor escolhido deve estar próximo ao parâmetro,
estudaremos utilidades do tipo 
$U(a,\theta) = -w(\theta)d(a,\theta)$,
onde $w(\theta)$ é uma função não-negativa que i
ndica a importância de acertar o valor $\theta$ e
$d(a,\theta)$ é uma distância entre $a$ e $\theta$.

Para algumas utilidades deste tipo é possível derivar
precisamente qual o melhor estimador.
A seguir, estudaremos alguns destes casos.

\subsubsection{Distância quadrática}

A distância quadrática, $d_{2}$, 
é tal que $d_{2}(a,\theta) = (a-\theta)^{2}$.
Esta é uma das funções mais frequentemente usadas 
em Teoria Estatística,
estando ligada à técnica dos mínimos quadrados.

O seguinte lema é útil para 
provar resultados envolvendo 
a distância quadrática
\begin{lemma}
 \label{lemma:conditional_l2}
 Sejam $\theta$ e $X$ 
 duas variáveis aleatórias,
 \begin{align*}
  \E[(\theta-f(X))^{2}|X]
  &= \V[\theta|X] + (\E[\theta|X]-f(X))^{2}
 \end{align*}
\end{lemma}
\begin{proof}
 \begin{align*}
  \E[(\theta-f(X))^{2}|X]
  &= \E[\theta^{2}|X] 
  -2\E[\theta f(X)|X] + \E[f(X)^{2}|X] \\
  &= \E[\theta^{2}|X] -2f(X)\E[\theta|X] + f(X)^{2} \\
  &= \E[\theta^{2}|X] -\E[\theta|X]^{2} +\E[\theta|X]^{2} 
  -2f(X)\E[\theta|X] + f(X)^{2}	\\
  &= \V[\theta|X] + (\E[\theta|X]-f(X))^{2}
 \end{align*}
\end{proof}
Usando o lema acima, podemos 
achar o melhor estimador para
a distância quadrática
\begin{theorem}
 \label{thm:estimation_l2}
 Seja $\hat{\theta}$ um estimador arbitrário.
 Se $U(\hat{\theta},\theta) = -(\hat{\theta}-\theta)^{2}$
 e existe $\hat{\theta}$ tal que 
 $\E[U(\hat{\theta},\theta)] > -\infty$,
 então o melhor estimador, $\hat{\theta}_{*}$, 
 é tal que
 \begin{align*}
  \hat{\theta}_{*} = \E[\theta|X]
 \end{align*}
\end{theorem}

\begin{proof}
\begin{align*}
 \hat{\theta}_{*}(X)
 &= \arg\max_{\hat{\theta} \in \mathcal{A}}
 \E[U(\hat{\theta},\theta)|X]
 & \text{\cref{theorem:extensive-form}} \\
 &= \arg\max_{\hat{\theta} \in \mathcal{A}}
 \E[-(\hat{\theta}-\theta)^{2}|X] \\
 &= \arg\max_{\hat{\theta} \in \mathcal{A}}
 -\V[\hat{\theta}-\theta|X] 
 -|\E[\theta|X]-\hat{\theta}|^{2}
 & \text{\cref{lemma:conditional_l2}} \\
 &= \arg\max_{\hat{\theta} \in \mathcal{A}}
 -\V[\theta|X] - |\E[\theta|X]-\hat{\theta}|^{2}
 &\hat{\theta} \text{ é constante dado X} \\
 &= \arg\max_{\hat{\theta} \in \mathcal{A}}
 -|\E[\theta|X]-\hat{\theta}|^{2} = \E[\theta|X]			  & \V[\theta|X] \text{ não depende de } \hat{\theta}
\end{align*}
\end{proof}

Assim, o melhor estimador segundo 
a distância quadrática é 
a média da distribuição 
\emph{a posteriori} do parâmetro.

Podemos generalizar o resultado acima para 
o caso multivariado. Considere que 
$\theta$ é um vetor de parâmetros reais.
Neste caso, a distância quadrática é 
generalizada em uma forma quadrática.
Ou seja, para alguma matriz positiva definida, $A$, 
$d_{2}(\hat{\theta},\theta)$ é
definida como 
$(\hat{\theta}-\theta)^{T}A(\hat{\theta}-\theta)$ e $U(\hat{\theta},\theta) = -d_{2}(\hat{\theta},\theta)$.
Neste caso, obtemos resultado semelhante 
ao \cref{lemma:conditional_l2}
\begin{lemma}
 \label{lemma:conditional_l2_multi}
 \begin{align*}
  \E[(\hat{\theta}-\theta)^{T}A(\hat{\theta}-\theta)|X]	
  &= \E[(\theta-E[\theta|X])^{T}A(\theta-\E[\theta|X])] 
  +(\hat{\theta}-\E[\theta|X])^{T}A(\hat{\theta}-\E[\theta|X])
 \end{align*}
\end{lemma}
\begin{proof}
 \begin{align*}
  \E[(\hat{\theta}-\theta)^{T}A(\hat{\theta}-\theta)|X]	
  &= \E[\theta^{T}A\theta|X]
  -\E[\hat{\theta}^{T}A\theta|X]
  -\E[\theta^{T}A\hat{\theta}|X]
  +\E[\hat{\theta}^{T}A\hat{\theta}|X] \\
  &= \E[\theta^{T}A\theta|X]-\hat{\theta}^{T}A\E[\theta|X]
  -\E[\theta^{T}|X]A\hat{\theta}
  +\hat{\theta}^{T}A\hat{\theta} \\
  &= \E[\theta^{T}A\theta|X]
  -\E[\theta|X]^{T}AE[\theta|X]	\\
  &+ \E[\theta|X]^{T}A\E[\theta|X]
  -\hat{\theta}^{T}A\E[\theta|X]
  -\E[\theta^{T}|X]A\hat{\theta}
  +\hat{\theta}^{T}A\hat{\theta} \\
  &= \E[\theta^{T}A\theta|X]
  -\E[\theta|X]^{T}AE[\theta|X]	\\
  &+ (\E[\theta|X]-\hat{\theta})^{T}A(\E[\theta|X]-\hat{\theta}) \\
  &= \E[(\theta-\E[\theta|X])^{T}A(\theta-\E[\theta|X])|X]
  +(\E[\theta|X]-\hat{\theta})^{T}A(\E[\theta|X]-\hat{\theta})
 \end{align*}
\end{proof}
A partir do \cref{lemma:conditional_l2_multi},
podemos provar
\begin{theorem}
 \label{thm:estimation_l2_multi}
 Seja $\hat{\theta}$ um estimador arbitrário.
 Se $U(\hat{\theta},\theta) = -(\hat{\theta}-\theta)^{T}A(\hat{\theta}-\theta)$, e 
 existe $\hat{\theta}$ tal que 
 $\E[U(\hat{\theta},\theta)] > -\infty$,
 então o melhor estimador, $\hat{\theta}_{*}$, 
 é tal que
 \begin{align*}
  \hat{\theta}_{*}	&= \E[\theta|X]
 \end{align*}
\end{theorem}

\begin{proof}
 \begin{align*}
  \hat{\theta}_{*}
  &= \arg\max_{\hat{\theta} \in \mathcal{A}}
  \E[U(\hat{\theta},\theta)|X]
  & \text{\cref{theorem:extensive-form}} \\
  &= \arg\max_{\hat{\theta} \in \mathcal{A}}
  -\E[(\theta-\hat{\theta})^{T}A(\theta-\hat{\theta})|X] \\
  &= \arg\max_{\hat{\theta} \in \mathcal{A}}
  -\E[(\theta-\E[\theta|X])^{T}A(\theta-\E[\theta|X])|X]
  -(\E[\theta|X]-\hat{\theta})^{T}A(\E[\theta|X]-\hat{\theta})	
  & \text{\cref{lemma:conditional_l2_multi}} \\
  &= \arg\max_{\hat{\theta} \in \mathcal{A}}
  -(\E[\theta|X]-\hat{\theta})^{T}A(\E[\theta|X]-\hat{\theta}) 
  = \E[\theta|X]
 \end{align*}
\end{proof}

\subsubsection{Desvio absoluto}

O desvio absoluto, $d_{1}$, é tal que 
$d_{1}(a,\theta) = |a-\theta|$.
Esta distância, historicamente, foi 
menos estudada em estatística devido
à maior dificuldade de obter resultados analíticos.
O desvio absoluto está tipicamente associado a 
estimadores robustos, ou seja, 
que não são fortemente influenciados por \emph{outliers}.
Isto ocorre pois, em relação à distância quadrática,
o desvio penaliza menos grandes desvios do estimador 
em relação a $\theta$.
Para o desvio absoluto, obtemos o seguinte resultado

\begin{theorem}
 \label{thm:estimation_l1}
 Seja $\hat{\theta}$ um estimador arbitrário.
 Se $U(\hat{\theta},\theta) = -|\hat{\theta}-\theta|$ e
 existe $\hat{\theta}$ tal que 
 $\E[U(\hat{\theta},\theta)] > -\infty$, então 
 o melhor estimador, $\hat{\theta}_{*}$ é tal que
 \begin{align*}
  \hat{\theta}_{*} = Med[\theta|X]
 \end{align*}
\end{theorem}

\begin{lemma}
 \label{lemma:estimation_l1}
 Defina $A^{-}_X = \{\omega \in \Omega: \theta < M_{X}\}$, 
 $A^{+}_X = \{\omega \in \Omega: \theta > M_{X}\}$, 
 $A^{=}_{X} = \{\omega \in \Omega: \theta = M_{X}\}$ e
 $M_X=Med[\theta|X]$. Obtém-se
 \begin{align*}
  \E[(|\hat{\theta}-\theta|-|M_{X}-\theta|)\I_{A^{-}_X}|X]
  &\geq (\hat{\theta}-M_{X})\P(A^{-}_X|X) \\
  \E[(|\hat{\theta}-\theta|-|M_{X}-\theta|)\I_{A^{+}_X}|X]
  &\geq -(\hat{\theta}-M_{X})\P(A^{+}_X|X) \\
  \E[(|\hat{\theta}-\theta|-|M_{X}-\theta|)\I_{A^{=}_X}|X]
  &\geq |\hat{\theta}-M_{X}|\P(\I_{A^{=}_X}|X)
 \end{align*}
\end{lemma}

\begin{proof}
 Note que
 \begin{align}
  \label{eq:estimation_l1_1}
  |\hat{\theta}-\theta| 
  &= \max(\hat{\theta}-\theta,\theta-\hat{\theta})
  \geq \hat{\theta}-\theta \nonumber \\
  |\hat{\theta}-\theta| 
  &= \max(\hat{\theta}-\theta,\theta-\hat{\theta})
  \geq \theta-\hat{\theta}
 \end{align}
 Portanto,
 \begin{align*}
  \E[(|\hat{\theta}-\theta|-|M_{X}-\theta|)\I_{A^{-}_X}|X]
  &=\E[(|\hat{\theta}-\theta|-M_{X}+\theta)\I_{A^{-}_X}|X]\\
  &\geq \E[(\hat{\theta}-\theta-M_{X}+\theta)\I_{A^{-}_X}|X]
  & \text{\cref{eq:estimation_l1_1}} \\
  &= (\hat{\theta}-M_{X})\E[\I_{A^{-}_X}|X] \\
  &= (\hat{\theta}-M_{X})\P(A^{-}_X|X)
 \end{align*}
 \begin{align*}
  \E[(|\hat{\theta}-\theta|-|M_{X}-\theta|)\I_{A^{+}_X}|X]
  &=\E[(|\hat{\theta}-\theta|+M_{X}-\theta)\I_{A^{+}_X}|X]\\
  &\geq \E[(-\hat{\theta}+\theta
  +M_{X}-\theta)\I_{A^{+}_X}|X]
  & \text{\cref{eq:estimation_l1_1}} \\
  &= (-\hat{\theta}+M_{X})\E[\I_{A^{+}_X}|X] \\
  &= -(\hat{\theta}-M_{X})\P(A^{+}_X|X)
 \end{align*}
 \begin{align*}
  \E[(|\hat{\theta}-\theta|-|M_{X}-\theta|)\I_{A^{=}_X}|X]
  &=\E[(|\hat{\theta}-\theta|)\I_{A^{=}_X}|X]\\
  &=\E[(|\hat{\theta}-M_{X}|)\I_{A^{=}_X}|X]\\
  &= |\hat{\theta}-M_{X}|\E[\I_{A^{=}_X}|X]\\
  &=|\hat{\theta}-M_{X}|\P(\I_{A^{=}_X}|X)
 \end{align*}
\end{proof}

\begin{proof}[Demonstração do \cref{thm:estimation_l1}]
 Defina $M_{X} := Med[\theta|X]$,
 $A^{-}_X = \{\omega \in \Omega: \theta < M_{X}\}$, 
 $A^{+}_X = \{\omega \in \Omega: \theta > M_{X}\}$ e 
 $A^{=}_{X} = \{\omega \in \Omega: \theta = M_{X}\}$.
 Note que
 \begin{align}
  \label{eq:thm_estimation_l1_1}
  \P(A^{=}_{X}|X)-|\P(A^{+}_{X}|X)-\P(A^{-}_{X}|X)| 
  &\geq 0
  & \text{$Med[\theta|X]$ é 
  a mediana a posteriori de $\theta$.}
 \end{align}
 Também, como 
 $\{A^{-}_X,A^{+}_X,A^{=}_X\}$ particiona $\Omega$,
 $\I_{A^{-}_X}+\I_{A^{+}_X}+\I_{A^{=}_X}=1$.
 Portanto,
 \begin{align*}
  \E[U(M_{X},\theta)|X] -\E[U(\hat{\theta},\theta)|X]
  =&\E[|M_{X}-\theta||X] +\E[|\hat{\theta}-\theta||X] \\
  =& \E[|\hat{\theta}-\theta|-|M_{X}-\theta||X] \\
  =& \E[(|\hat{\theta}-\theta|-|M_{X}-\theta|)(\I_{A^{-}_X}+\I_{A^{+}_X}+\I_{A^{=}_X})|X] \\
  =& \E[(|\hat{\theta}-\theta|-|M_{X}-\theta|)\I_{A^{-}_X}|X]
 +\E[(|\hat{\theta}-\theta|
 -|M_{X}-\theta|)\I_{A^{+}_X}|X]\\
 &+\E[(|\hat{\theta}
 -\theta|-|M_{X}-\theta|)\I_{A^{=}_X}|X]\\
 &\geq (\hat{\theta}-M_X)(\P(A^-_X|X)-\P(A^+_X|X)
 +|\hat{\theta}-M_X|\P(A^=_X|X)
 & \text{\cref{lemma:estimation_l1}} \\
 &\geq -|\hat{\theta}-M_X||(\P(A^-_X|X)-\P(A^+_X|X)|
 +|\hat{\theta}-M_X|\P(A^=_X|X) \\
 &= |\hat{\theta}-M_X|(\P(A^=_X|X)
 -|(\P(A^-_X|X)-\P(A^+_X|X)|) \geq 0
 & \text{\cref{eq:thm_estimation_l1_1}}
 \end{align*}
 Portanto, como 
 $\E[U(M_X,\theta)|X] \geq \E[U(\hat{\theta},\theta)|X]$,
 decorre do \cref{theorem:extensive-form} que
 $\text{Med}[\theta|X]$ é o melhor estimador para 
 $\theta$ sob a utilidade $-d_{1}(\hat{\theta},\theta)$.
\end{proof}

\subsubsection*{Exercícios}

\begin{exercise}
 Defina $M_{X} := Med[\theta|X]$,
 $A^{-}_X = \{\omega \in \Omega: \theta < M_{X}\}$, 
 $A^{+}_X = \{\omega \in \Omega: \theta > M_{X}\}$ e 
 $A^{=}_{X} = \{\omega \in \Omega: \theta = M_{X}\}$.
 Prove que
 \begin{align*}
  \P(A^{=}_{X}|X)
  -|\P(A^{-}_{X}|X)-\P(A^{+}_{X}|X)| 
  &\geq 0
 \end{align*}
\end{exercise}

\begin{exercise}
 Considere que, dado $\theta$, 
 $X_{1},\ldots,X_{n}$ são i.i.d. e
 $X_{i} \sim \text{Binomial}(m, \theta)$.
 Se \emph{a priori},
 $\theta \sim \text{Beta}(\alpha,\beta)$,
 \begin{enumerate}[label=(\alph*)]
  \item Ache $\hat{\theta}^{*}_{2}$, 
  o melhor estimador para $\theta$ sob
  $U(\hat{\theta},\theta)=-d_{2}(\hat{\theta},\theta)$.
  \item Ache $\lim_{n}\hat{\theta}^{*}_{2}$.
  \item Compare a utilidade esperada a posteriori de
  $\hat{\theta}^{*}_{2}$ e $m^{-1}\bar{X}$ sob
  $U(\hat{\theta},\theta)=-d_{2}(\hat{\theta},\theta)$.
 \end{enumerate}
\end{exercise}

\solution{\textbf{Solução}:
 \begin{enumerate}[label=(\alph*)]
  \item Sabemos que $\theta|X_{1},\ldots,X_{n} \sim \text{Beta}(\alpha+n\bar{X},\beta+nm-n\bar{X})$.
  Ademais, segue do \cref{thm:estimation_l2} que
  \begin{align*}
   \hat{\theta}^{*}_{2}	
   &= \E[\theta|X] \\
   &= \frac{\alpha+n\bar{X}}{\alpha+\beta+nm}
  \end{align*}
  \item
  \begin{align*}
   \lim_{n \rightarrow \infty}\hat{\theta}^{*}_{2}
   &= \lim_{n \rightarrow \infty}\frac{\alpha+n\bar{X}}{\alpha+\beta+nm} \\
   &= \lim_{n \rightarrow \infty}\frac{\bar{X}}{m} \\
   &= \frac{m\theta}{m} = \theta							    & \text{lei dos grandes números}
  \end{align*}
  \item 
  \begin{align*}
   \E[U(m^{-1}\bar{X},\theta)|X]
   &= \E[U(m^{-1}\bar{X},\theta)|X] \\
   &= \E[-(m^{-1}\bar{X}-\theta)^2|X] \\
   &= -\V[\theta-m^{-1}\bar{X}|X]
   -(\E[\theta|X]-m^{-1}\bar{X})^{2} \\
   &= -\V[\theta|X]]
   -(\E[\theta|X]-m^{-1}\bar{X})^{2}
  \end{align*}
  Similarmente,
  \begin{align*}
   \E[U(\E[\theta|X],\theta)|X]	
   &= \E[U(\E[\theta|X],\theta)|X] \\
   &= \E[-(\E[\theta|X]-\theta)^2|X] \\
   &= -\V[\theta-\E[\theta|X]|X]
   -(\E[\theta|X]-\E[\theta|X])^{2} \\
   &= -\V[\theta|X]	
  \end{align*}
  Portanto,
  \begin{align*}
   \E[U(\E[\theta|X],\theta)|X]
   -\E[U(m^{-1}\bar{X},\theta)|X]
   &=(\E[\theta|X]-m^{-1}\bar{X})^{2} \\
   &=\left(\frac{\alpha+n\bar{X}}{\alpha+\beta+nm}
   -\frac{\bar{X}}{m}\right)^{2} \\
   &= \left(\frac{m\alpha-(\alpha+\beta)\bar{X}}{m(\alpha+\beta+nm)}\right)^{2}
  \end{align*}
 \end{enumerate}
}{}

\begin{exercise}
 Considere que, dado $\theta$, 
 $X_{1},\ldots,X_{n}$ são i.i.d. e
 $X_{i} \sim N(\theta,1)$. 
 Se \emph{a priori} $\mu \sim N(0,1)$,
 \begin{enumerate}[label=(\alph*)]
  \item Ache $\hat{\theta}^{*}_{2}$, 
  o melhor estimador para $\theta$ sob 
  $U(\hat{\theta},\theta)=-d_{2}(\hat{\theta},\theta)$.
  \item Ache $\lim_{n}\hat{\theta}^{*}_{2}$.
  \item Compare a utilidade esperada a posteriori 
  de $\hat{\theta}^{*}_{2}$ e $\bar{X}$ sob
  $U(\hat{\theta},\theta)=-d_{2}(\hat{\theta},\theta)$.
  \item Ache $\hat{\theta}^{*}_{1}$, 
  o melhor estimador para $\theta$ sob 
  $U(\hat{\theta},\theta)=-d_{1}(\hat{\theta},\theta)$.
 \end{enumerate}
\end{exercise}

\solution{\textbf{Solução}: Sabemos que
 $\theta|X_{1},\ldots,X_{n} \sim \text{N}(\frac{n\bar{X}}{n+1},n+1)$.
 \begin{enumerate}[label=(\alph*)]
  \item Segue do \cref{thm:estimation_l2} que
  \begin{align*}
   \hat{\theta}^{*}_{2}	&= \E[\theta|X] \\
   &= \frac{n\bar{X}}{n+1}
  \end{align*}
  \item
  \begin{align*}
   \lim_{n}\hat{\theta}^{*}_{2}	
   &= \lim_{n}\frac{n\bar{X}}{n+1} \\
   &= \lim_{n}\bar{X} = \theta
   & \text{lei dos grandes números}
  \end{align*}
  \item
  \begin{align*}
   \E[U(\bar{X},\theta)|X]
   &= \E[U(\bar{X},\theta)|X] \\
   &= \E[-(\bar{X}-\theta)^2|X] \\
   &= -\V[\theta-\bar{X}|X] 
   -(\E[\theta|X]-\bar{X})^{2} \\
   &= -\V[\theta|X] -(\E[\theta|X]-\bar{X})^{2}
  \end{align*}
  Similarmente,
  \begin{align*}
   \E[U(\E[\theta|X],\theta)|X]
   &= \E[U(\E[\theta|X],\theta)|X] \\
   &= \E[-(\E[\theta|X]-\theta)^2|X] \\
   &= -\V[\theta-\E[\theta|X]|X] 
   -(\E[\theta|X]-\E[\theta|X])^{2}	\\
   &= -\V[\theta|X]	
  \end{align*}
  Portanto,
  \begin{align*}
   \E[U(\E[\theta|X],\theta)|X]
   -\E[U(\bar{X},\theta)|X]	
   &= (\E[\theta|X]-\bar{X})^{2} \\
   &= \left(\frac{n\bar{X}}{n+1} - \bar{X}\right)^{2} \\
   &= \frac{\bar{X}^{2}}{(n+1)^{2}}
  \end{align*}
  \item Segue do \cref{thm:estimation_l1} que
  \begin{align*}
   \hat{\theta}^{*}_{1}	&= Med[\theta|X] \\
   &= \frac{n\bar{X}}{n+1}
  \end{align*}
 \end{enumerate}
}{}

\begin{exercise}
 Considere que a proporção de 
 indivíduos doentes em uma determinada 
 população é um valor desconhecido, $\theta$.
 Uma amostra de $n$ indivíduos é 
 retirada independentemente da população.
 Para cada indivíduo a probabilidade de que 
 o exame seja positivo dado que 
 o indivíduo está doente é $\alpha$.
 A probabilidade de que o exame seja 
 negativo dado que o indivíduo não está 
 doente é $\beta$. Dos $n$ indivíduos testados, 
 $n\bar{x}$ tiveram o teste positivo.
 Considere que, \emph{a priori}, 
 $\theta \sim U(0,1)$. Ache o melhor estimador para 
 $\theta$ de acordo com 
 $U(\hat{\theta},\theta) = -d_{2}(\hat{\theta},\theta)$.
\end{exercise}

\solution{\textbf{Solução}: Defina
 \begin{align*}
  \begin{cases}
   X_{i} & \text{a indicadora de que 
   o exame do i-ésimo indivíduo foi positivo, } 
   X_i \in \{0,1\} \\
   Z_{i} & \text{a indicadora de que 
   o i-ésimo indivíduo está doente, } 
   Z_i \in \{0,1\}
  \end{cases}
 \end{align*}
 Note que
 \begin{align*}
  \P(X_{i}=1|\theta)
  &= \P(X_{i}=1,Z_{i}=0|\theta) 
  +\P(X_{i}=1,Z_{i}=1|\theta) \\
  &= \P(Z_{i}=0|\theta)\P(X_{i}=1|Z_{i}=0,\theta)
  +\P(Z_{i}=1|\theta)\P(X_{i}=1|Z_{i}=1,\theta)	\\
  &= (1-\theta)(1-\beta) + \theta \alpha 
  = (1-\beta) + (\alpha+\beta-1)\theta
 \end{align*}
 Portanto,
 \begin{align*}
  f(\theta|x_{1},\ldots,x_{n})
  &\propto f(\theta)f(x_{1},\ldots,x_{n}|\theta) \\
  &= 1 \cdot \prod_{i=1}^{n}{f(x_{i}|\theta)} \\ 
  &= \prod_{i=1}^{n}\left((1-\beta) 
  +(\alpha+\beta-1)\theta\right)^{x_{i}}
  \left(\beta -(\alpha+\beta-1)\theta\right)^{1-x_{i}} \\
  &= \left((1-\beta) 
  +(\alpha+\beta-1)\theta\right)^{n\bar{x}}
  \left(\beta-(\alpha+\beta-1)\theta\right)^{n-n\bar{x}}
 \end{align*}
 Note que 
 $(1-\beta)+(\alpha+\beta-1)\theta|X \sim \text{Beta}(n\bar{X}+1,n-n\bar{X}+1)$.
 Portanto,
 \begin{align*}
  \E\left[(1-\beta)+(\alpha+\beta-1)\theta|X\right]
  &= \frac{n\bar{X}+1}{n+2}	\\
  (1-\beta) + (\alpha+\beta-1)\E[\theta|X]
  &= \frac{n\bar{X}+1}{n+2}	\\
  \E[\theta|X]
  &= \frac{\frac{n\bar{X}+1}{n+2} 
  -(1-\beta)}{\alpha+\beta-1}
 \end{align*}
}{}

\begin{exercise}[{\citet[p.309]{Schervish2012}}]
 Seja $0 < c < 1$. Considere um 
 problema de estimação sob a utilidade:
 \begin{align*}
  U(\hat{\theta},\theta) &=
  \begin{cases}
   -c(\hat{\theta}-\theta) 
   & \text{, se } \hat{\theta} \geq \theta \\
   -(1-c)(\theta-\hat{\theta}) 
   & \text{, se } \hat{\theta} < \theta
  \end{cases}
 \end{align*}
 Determine o estimador ótimo.
\end{exercise}

